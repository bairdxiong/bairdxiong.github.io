<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Bing Xiong, Xiong Bing, SIAT"> 
<meta name="description" content="BingXiong's Home">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Bing Xiong@SIAT</title>
<style>
    .smaller-image {
      width: 20%;
    }
</style>


</head>
<body>
<div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/bairdxiong" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Bing(Baird) Xiong </h1>
					<h1></h1>
					<!-- <img src="indexpics/sign.png" alt="你的图片"  class="smaller-image"> -->
					</div>
				<h3>Master Student</h3>
				<p>
					ShenZhen Institues of Advanced Technology(SIAT)<br>
					ShenZhen.<br>
					<br>
					Email: b.xiong[at]siat.ac.cn<br>
					
				</p>
				<p> <a href="https://scholar.google.com/citations?user=4LPt-bMAAAAJ&hl=zh-CN"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://www.linkedin.com/in/yue-ma-a991b425a/"><img src="./pic/LinkedIn.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/bairdxiong"><img src="./pic/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
					
				</p>
			</td>
			<td>
				<!-- <img src="./pic/my3.jpg" border="0" width="240"><br> -->
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Short Bio</h2>
<p>
	I am a second-year Master student of CS at The ShenZhen Institues of Advanced Technology (SIAT)</a>, under the supervision of <a href="https://cqf.io/" target="_blank">Prof. Wenjian Qin</a>. 
	I obtained my  B.Eng in Computer Science at Wuhan University of Technology</a> in 2023. 
	Currently, I am a research intern at Deepwise, supervised by <a href="https://ieeexplore.ieee.org/author/37086384667" target="_blank">Dr. Zhen Zhou</a> and <a href="https://scholar.google.com/citations?user=e38fTZQAAAAJ&hl=zh-CN">Prof.YiZhou Yu(IEEE Fellow)</a>  . 
</p>
<p>My research interests lie in the intersection of <b>Computer Vision</b> and  <b>Machine Learning</b>. From 2022, I started to do some research on digital pathology and medical imaging. Now, 
	I focus on designing novel applications for three dimension hitopathology reconstruction and medical image imaging and downstream task and virtual staining.</p>

<p><i style="color: red; display: inline;">Feel free to contact me by email if you are interested in discussing or collaborating with me.</i></p>


<h2>News</h2>
<div style="height: 240px; overflow: auto;">
<ul>
	<li>
		[01/2025] Recipient of SIAT Instrument Cheif Innovation Fund Scholarship - Student Award
	<li>
		[01/2025] Recipient of SIAT 2024 Outstanding Student Scholarship - Grade 1
	</li>
	<li>
		[12/2024] Recipient of the SIAT 2024 Excellent Student Cadre Award
	</li>
	<li>
		[12/2024] Joined <a href="https://www.deepwise.com">Deepwise Research</a> as research intern. Started doing research on Medical Imaging !
	</li> 
	<li>
		[12/2024] One paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a> ! 
	</li> 
	<li>
		[4/2024] One paper are accepted by <a href="https://www.sciencedirect.com/journal/displays">Displays</a> ! 
	</li> 
	<!-- <li>
		[12/2024] Five paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a> !  
	</li>
	<li>
		[09/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">WACV 2025</a> !  
	</li>
	<li>
		[08/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">NeurIPS 2024</a> !  
	</li>
	<li>
		[07/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">Siggraph Asia 2024</a> !  
	</li>
	<li>
		[07/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">ACM MM 2024</a> !  
	</li>
	<li>
		[06/2024] Excited to release <i style="color: red; display: inline;">Follow-Your-Emoji</i>, a freestyle portrait animation framework. Enjoy <a href="https://github.com/mayuelala"> FollowYourEmoji</a> quickly !
	</li>
	<li>
		[03/2024] Excited to release <i style="color: red; display: inline;">Follow-Your-Click</i>, the first framework to achieve regional image animation. Enjoy the fun of <a href="https://github.com/mayuelala/FollowYourClick">FollowYourClick</a> !
	</li>
	<li>
		[01/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">CVPR 2024</a> !  
	</li>
	<li>
		[12/2023] Two paper are accepted by <a href="https://aaai.org/aaai-conference/">AAAI 2024</a> !  
	</li>
	<li>
		[11/2023]  Excited to release <i style="color: red; display: inline;">MagicStick</i>, the first unified framework to modify video properties leveraging the keyframe transformations on the extracted internal control signals.  
	</li>
	<li>
		[10/2023] One paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM 2024</a> !
	</li>
	<li>
		[09/2023] I am serving as a Reviewer for  <a href="https://iclr.cc/">ICLR 2024</a> !
	</li>
	<li>
		[07/2023] I start to cooperate with <a href="https://scholar.google.com/citations?user=lLMX9hcAAAAJ">Qifeng Chen</a> closely in <a href="https://cse.hkust.edu.hk/">HKUST</a> !
	</li>
	<li>
		[06/2023] I end my research intership in <a href="https://ai.tencent.com/">Tencent AI Lab</a> !
	</li>
	<li>
		[06/2023] I am serving as a Reviewer for  <a href="https://nips.cc/">NeurIPS 2023</a> !
	</li>
	<li>
		[05/2023] I am serving as a Reviewer for  <a href="https://www.acmmm2023.org/">ACM MM 2023</a> !
	</li>
	<li>
		[04/2023] Excited to release <i style="color: red; display: inline;">Follow-Your-Pose</i> framework, generating the character videos from pose and text description. Enjoy the fun of <a href="https://github.com/mayuelala/FollowYourPose">FollowYourPose</a> !
	</li>
	<li>
		[03/2023] Two paper are submitted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a> !
	</li>
	<li>
		[02/2023] One paper <a herf="https://arxiv.org/abs/2302.05940">SemanticAC</a> about Audio Classification was accepted by <a herf="https://2023.ieeeicassp.org/">ICASSP 2023</a> !
	</li>
	<li>
		[12/2022] Awarded First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">Tsinghua Shenzhen International Graduate School</a> !
	</li>
	<li>
		[11/2022] One paper about video-text pretraining with masked autoencoder is submitted to  <a href="https://cvpr2023.thecvf.com/CVPR">CVPR 2023</a> !
	</li>
	<li>
		[10/2022] One paper about semantics-assisted framework for audio classification is submitted to <a href="https://2023.ieeeicassp.org/">ICASSP 2023</a> !
	</li>
	<li>
		[09/2022] Attending <a href="https://2022.acmmm.org/">ACM MM 2022</a>, Welcome to chat!
	</li>
	<li>
		[09/2022] <a href="https://">My first paper</a> has been accepted for <i style="color: red; display: inline;">Oral Presentation</i> on ACM MM 2022 (Top 5%) !
	</li>
	<li>
		[05/2022] Joined <a href="https://ai.tencent.com/">Tencent AI Lab</a> as research intern !
	</li>
	<li>
		[03/2022] Awarded <a href="https://www.withzz.com/project/detail/99">Tencent Rhino-Bird Research Elite Program</a> from Tencent ! Only <i style="color: red; display: inline;">72</i> students in the world admitted to this program !
	</li>
	<li>
		[07/2021] Joined <a href="http://mmlab.siat.ac.cn/">CUHK@MMLab in ShenZhen</a> as research intern !
	</li>
	<li>
		[06/2021] Recommended to <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a> towards the MSc degree !
	</li>
	<li>
		[06/2021] Graduated from <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a> !
	</li>
	<li>
		[01/2021] Joined <a href="https://www.tsinghua.edu.cn/">Baidu Research</a> as research intern. Started doing research on Computer Vision !
	</li> -->
</ul>
</div>

<h2>Industrial Experience</h2>
<table id="tbPublications" width="100%">
	<tbody>
	<p></p>
	</td>
	<tr>
	<tr>
		<td width="306">
		<img src="./industrial/deepwise.png" width="255px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b> Deepwise AI Lab</b></p>
		<p>Dec. 2024 - April.2025, Deepwise AI Lab, Beijing, China </p>
		<p>worked with <a href="https://ieeexplore.ieee.org/author/37086384667" target="_blank">Dr. Zhen Zhou</a> and <a href="https://scholar.google.com/citations?user=e38fTZQAAAAJ&hl=zh-CN">Prof.YiZhou Yu(IEEE Fellow)</a></p>
		<em>Topic: CT/PET Image Synthesis&Translation</em>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
</tbody></table>


<!-- The University of Sydney, Australia

Doctor of Philosophy (Ph.D.) Candidate in Computer Science
Team: Multimedia Laboratory, Sydney (MMLab@Sydney)
 Prof. Wanli Ouyang, Prof. Chang Xu
2022 - Present -->



<h2>Education & Visiting</h2>
<table id="tbPublications" width="100%">
	<!-- <tbody>
		<td width="306">
		<img src="./education/hkust.png" width="270px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b>The Hong Kong University of Science and Technology, Hong Kong</b></p>
		<p>PhD Student in Visual Intelligence Lab, HKUST </p>
		<p>Advisor: <a href="https://cqf.io/">Prof. Qifeng Chen</a> </p>
		<p>Sep. 2024 - Future <p>
		</p>
		</td>
	</tr>
	########################
	<!-- <tbody>
		<td width="306">
		<img src="./education/hkust.png" width="270px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b>The Hong Kong University of Science and Technology, Hong Kong</b></p>
		<p>Research Assistant in Visual Intelligence Lab, HKUST </p>
		<p>Advisor: <a href="https://cqf.io/">Prof. Qifeng Chen</a> </p>
		<p>July. 2023 - Nov. 2023 <p>
		</p>
		</td>
	</tr> -->
	<!--########################-->
	<p></p>
	</td>
	<tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr> 
	<tr>
		<td width="306">
		<img src="./education/siat.png" width="210px" style="box-shadow: 4px 4px 8px #ffffff" >	
		</td>
			
		<td>
		<p><b>University of Chinese Academy of Sciences, China</b></p>
		<p>Master of Computer Science </p>
		<p>Advisor: <a href="https://scholar.google.com/citations?user=Xrh1OIUAAAAJ&hl=zh-CN">Prof.Wenjian Qin</a></p>
		<p>Sep. 2023 - Future <p>
		</p>
		</td>
	</tr>

	<!--########################-->
		<p></p>
	</td>
	<tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./education/siat.png" width="210px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b> University of Chinese Academy of Sciences, China</b></p>
		<p>Research Assistant in Multimodal Intelligent Laboratory, SIAT, CAS (<a href="https://qin-mixlab.cn/">MixLab@SIAT</a>) </p>
		<p>Advisor: <a href="https://scholar.google.com/citations?user=QulpzUAAAAAJ&hl=zh-CN&oi=ao">Prof. Wenjian Qin</a> and  <a href="https://scholar.google.com/citations?user=KfrVZewAAAAJ&hl=zh-CN&oi=ao">Prof. YaoQin Xie</a></p>
		<p>Nov. 2022 - Aug. 2023 <p>
		</p>
		</td>
	</tr>
	<!--########################-->
	<tr>
		<tr>
			<td width="306">
			<img src="./education/whut.png" width="210px" style="box-shadow: 4px 4px 8px #ffffff">
			</td>				
			<td>
			<p><b>Wuhan University of technology, China</b></p>
			<p>Bachelor of Intelligent Manufacturing Engineering </p>
			<p>Sep. 2019 - Jun. 2023 <p>
			</p>
			</td>
		</tr>
		<tr>&nbsp</tr>

	<!--########################-->
		
</tbody></table>

<h2> Selected Publications | <a href="https://scholar.google.com/citations?user=4LPt-bMAAAAJ&hl=zh-CN">Full List</a></h2>
<!--
<div style="height: 1440px; overflow: auto;">
-->
<table id="tbPublications" width="100%">
	<!-- <tbody>
	<td><b>/*Preprints*/</b>
	<p></p>
	</td>
	<tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2024-arXiv-instantswap.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>InstantSwap: Fast Customized Concept Swapping across Sharp Shape Differences</b></p>
		<p>Chenyang Zhu*, Kai Li*, <b>Yue Ma*</b>, Longxiang Tang, Chengyu Fang, Chubin Chen, Qifeng Chen, Xiu Li</p>
		<em>arXiv preprint:2412.01197</em>
		<p> [<a href="hhttps://arxiv.org/abs/2412.01197">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2024-arXiv-RF-edit.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Taming Rectified Flow for Inversion and Editing</b></p>
		<p>Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, <b>Yue Ma</b>, Nisha Huang, Yuxin Chen, Xiu Li, Ying Shan</p>
		<em>arXiv preprint:2411.04746</em>
		<p> [<a href="hhttps://arxiv.org/abs/2411.04746">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2024-Fypv2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control</b></p>
		<p>Jingyun Xue, Hongfa Wang, Qi Tian, <b>Yue Ma</b>, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, Wei Liu, Mengyang Liu, Wenhan Luo</p>
		<em>arXiv preprint:2406.03035. 2024</em>
		<p> [<a href="hhttps://arxiv.org/abs/2406.03035">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2022-simvtp-framework.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>SimVTP: Simple Video Text Pre-training with Masked Autoencoders</b></p>
		<p><b>Yue Ma</b>, Tianyu Yang, Ying Shan, Xiu Li</p>
		<em>arXiv preprint:2211.03490. 2022</em>
		<p> [<a href="https://arxiv.org/pdf/2211.03490">paper</a>] [<a href="https://github.com/mayuelala/SimVTP">code</a>] </p>
		</td>
	</tr> --> 
	<!--########################-->
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<td><b>/*Conference*/</b>
	<p></p>
	</td>
	
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2024-DiT4Edit.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>DiT4Edit: Diffusion Transformer for Image Editing</b></p>
		<p>Kunyu Feng, <b>Yue Ma*</b>, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, Zeyu Wang</p>
		<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
		<p> [<a href="hhttps://arxiv.org/abs/2406.03035">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2024-Canvas.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</b></p>
		<p>Qihua Chen*, <b>Yue Ma*</b>, Hongfa Wang*, Junkun Yuan*, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, Wei Liu</p>
		<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
		<p> [<a href="hhttps://arxiv.org/abs/2409.01055">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2024-multiBooth.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>MultiBooth: Towards Generating All Your Concepts in an Image from Text</b></p>
		<p>Chenyang Zhu, Kai Li, <b>Yue Ma</b>, Chunming He, Xiu Li</p>
		<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
		<p> [<a href="hhttps://arxiv.org/abs/2404.14239">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
<tr>
	<td width="306">
	<img src="./indexpics/2024-followyourclick.png" width="285px" style="box-shadow: 4px 4px 8px #888">
	</td>				
	<td>
	<p><b>Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts</b></p>
	<p><b>Yue Ma</b>, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
	<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
	<p> [<a href="https://arxiv.org/abs/2403.08268">paper</a>] [<a href="https://github.com/mayuelala/FollowYourClick">code</a>] [<a href="https://follow-your-click.github.io/">project page</a>] 
	</p>
	</td>
</tr>
<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="./indexpics/2023-MagicStick.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>MagicStick🪄: Controllable Video Editing via Control Handle Transformations</b></p>
		<p><b>Yue Ma</b>, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng Chen</p>
		<em>IEEE /CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2025</em>
		<p> [<a href="https://arxiv.org/abs/2312.03047">paper</a>] [<a href="https://github.com/mayuelala/MagicStick">code</a>] [<a href="https://magic-stick-edit.github.io/">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	<!-- <tr></tr>
		<td width="306">
		<img src="./indexpics/2024-followyouremoji.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</b></p>
		<p><b>Yue Ma</b>, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
		<em>The ACM Special Interest Group for Computer Graphics and Interactive Techniques(<b>Siggraph Asia</b>) 2024</em>
		<p> [<a href="https://arxiv.org/abs/2406.01900">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://follow-your-emoji.github.io/">project page</a>] 
		</p>
		</td>
	</tr>
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2024-cove.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>COVE: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing</b></p>
		<p>Jiangshan Wang*, <b>Yue Ma*</b>, Jiayi Guo*, Yicheng Xiao, Gao Huang, Xiu Li</p>
		<em>Conference on Neural Information Processing Systems(<b>NeurIPS</b>). 2024</em>
		<p> [<a href="https://arxiv.org/abs/2406.08850">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://cove-video.github.io/">project page</a>] 
		</p>
		</td>
	</tr> -->
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2024-ACMMM.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Freehand Sketch Generation from Mechanical Components</b></p>
		<p>Zhichao Liao, Di Huang, Heming Fang, <b>Yue Ma</b>, Fengyuan Piao, Xinghui Li, Long Zeng, Pingfa Feng</p>
		<em>The 32th ACM International Conference on Multimedia. (<b>ACM MM</b>), 2024.</em>
		<p> [<a href="https://arxiv.org/abs/2406.08850">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://cove-video.github.io/">project page</a>] 
		</p>
		</td>
	</tr> -->
	<!--########################-->
	 <!-- <tr>
		<td width="306">
		<img src="./indexpics/2023-bridge.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection</b></p>
		<p>Yicheng Xiao, Zhuoyan Luo, Yong Liu, <b>Yue Ma</b>, Hengwei Bian, Yatai Ji, Yujiu Yang, Xiu Li</p>
		<em>IEEE /CVF Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024</em>
		<p> [<a href="https://arxiv.org/abs/2311.16464">paper</a>] [<a href="https://github.com/EasonXiao-888/UVCOM">code</a>] [<a href="https://github.com/EasonXiao-888/UVCOM">project page</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr> -->
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2023-iccv-followyourpose.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>🕺🕺🕺 Follow-Your-Pose 💃💃💃: Pose-Guided Text-to-Video Generation using Pose-Free Videos</b></p>
		<p><b>Yue Ma</b>, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen</p>
		<em>The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</em>
		<p> [<a href="https://arxiv.org/abs/2304.01186">paper</a>] [<a href="https://github.com/mayuelala/FollowYourPose">code</a>] [<a href="https://follow-your-pose.github.io/">project page</a>] 
		<a target="_blank" href ="https://github.com/mayuelala/FollowYourPose"><img alt="GitHub stars" align="right" src="https://img.shields.io/github/stars/mayuelala/FollowYourPose?style=social"></a></p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr> --> 
	<!--########################-->
	<tr>
		<td width="306">
		<img src="./indexpics/25-AAAI-stainprompt.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Unpaired Multi-Domain Histopathology Virtual Staining using Dual Path Prompted Inversion</b></p>
		<p> <b>Bing Xiong</b>, Yue Peng,Ranran Zhang,Fuqiang Chen, JiaYe He,Wenjian Qin</p>
		<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI2025</b>), <span style="color: red;">CCFA</span></em> 
		<p> [<a href="https://arxiv.org/abs/2412.11106">paper</a>] [<a href="https://github.com/bairdxiong/StainPromptInversion">code</a>] 
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!-- s -->
	<tr>&nbsp</tr>
    	<tr>&nbsp</tr>
    	<tr>&nbsp</tr>
		
<!-- 	<td><b>/*Journal*/</b>
		<p></p>
		</td>
		
		<tr>
			<td width="306">
			<img src="./indexpics/2024-Displays-agwnet.png" width="285px" style="box-shadow: 4px 4px 8px #888">
			</td>				
			<td>
			<p><b>AGWNet: Attention-guided adaptive shuffle channel gate warped feature network for indoor scene RGB-D semantic segmentation</b></p>
			<p><b>Bing Xiong</b>, Yue Peng,JingKe Zhu,Jia Gu,Zhen Chen, Wenjian Qin</p>
			<em>(<b>Displyas</b>), 2024.  <b>JCR Q1</b></em> 
			<i></i>
			<p> [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0141938224000945">paper</a>]  </p>
			</td>
		</tr>
		<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr> -->
	

</tbody></table>
<!--
</div>
-->






<h2>Selected Honors &amp; Awards</h2>
    <ul style="line-height: 1.8;">
	<li>
	[05/2025] Recipient of Universty of Chinese Academy of Sciences Model Student of Outstanding Capacity
	<p>（中国科学院大学（UCAS）三好学生标兵，<b>9 Students in the Institute</b>）</p>
	</li>
	<li>
    	[01/2025] Recipient of SIAT excellent student cadre
    	<p>（深圳先进技术研究院（SIAT）优秀学生干部，<b>22 Students in the Institute</b>）</p>
	</li>    
	<li>
    	[01/2025] Recipient of SIAT Institute of Scientific Instrumentation Cheif Innovation Fund Scholarship - Student Award
    	<p>（深圳先进技术研究院（SIAT）仪器所所长创新基金奖学金-优秀学生奖，<b>1 Master Students in the Institute</b>）</p>
	</li>
        <li>
            [01/2025] Recipient of SIAT Outstanding Student Scholarship - Grade 1
            <p>（深圳先进技术研究院（SIAT）优秀学生一等奖学金，<b>10 Master Students in the Institue,SIAT</b>）</p>
        </li>
        <li>
            [11/2024] UCAS 2024 Postgraduate Academic Forum Bay Area Forum <b>Excellence Award</b>
            <p>（中国科学院大学 2024 年研究生学术论坛（湾区论坛）优秀奖<b>，8 Students in the Institue,SIAT</b>） </p>
        </li>
        <li>
            [11/2024] <b>National Scholarship</b> of Shenzhen Institutes of Advanced Technology, UCAS
            <p>（中国科学院大学深圳先进技术研究院国家奖学金，<b style="color: red;">Top 0.3% in ShenZhen Institute of Advanced Technology</b> ）</p>
        </li>
        <li>
            [06/2023] Outstanding Undergraduate extracurricular training student of Wuhan University of Technology
            <p>（武汉理工大学本科生课外优秀训练学生）</p>
        </li>
        <li>
            [06/2023] Outstanding graduates of Wuhan University of Technology
            <p>（武汉理工大学优秀毕业生）</p>
        </li>
        <li>
            [12/2022] PaddleClub <b>Gold Medal Team Leader</b> of Baidu
            <p>（百度飞桨领航团 金牌团长）</p>
        </li>
        <li>
            [12/2022] PaddlePaddle <b><a href="https://github.com/PaddlePaddle/PaddleDepth">Excellent open source project in Aistudio</a></b> of Baidu
            <p>（百度 PaddlePaddle年度峰会 优秀开源项目）</p>
        </li>
        <li>
            [12/2022] PaddlePaddle Aistudio <b>Top 10 Most Influential People</b> of Baidu, only 10 developers in the world admitted to this program
            <p>（入选百度 PaddlePaddle <b>Aistudio2022年 十大最具影响力开发者</b>）</p>
        </li>
        <li>
            [12/2022] Second Prize in the National Finals of the National College Student Software Design Competition
            <p>（全软件设计大赛智能遥感解译赛道全国总决赛二等奖）</p>
        </li>
        <li>
            [08/2022] National Second Prize, Provincial First and Third Prize in the National College Student Smart Car Competition 
            <p>（全国大学生智能车竞赛国家二等奖、省一等奖和三等奖）</p>
        </li>
        <li>
            [07/2022] National Second Prize in the National College Student Energy Conservation&Emission Reduction Innovation Competition
            <p>（全国大学生节能减排创新大赛国家二等奖）</p>
        </li>
        <li>
            [04/2022] PaddlePaddle Developer Expert (PPDE) of Baidu
            <p>（被授予百度 PaddlePaddle 开发者专家（PPDE）称号）</p>
        </li>
        
    </ul>



<h2>Invited Talk</h2>
<ul>
	<li>	
	2024.9.22 Talk at <a herf="http://www.csbme.org">China Biomedical Engineering Conference & Medical Innovation Summit</a>(BME2024)<br>
	</li>


</ul>


<h2>Professional Services</h2>
<ul>
	<li>	
	<b>Jounral Reviewer:</b>
	<br>The International Journal of Research on Intelligent Systems for Real Life Complex Problems (Applied Intelligence)<br>
	</li>

	<li>	
	<b>Conference Reviewer:</b><br>
	International Joint Conference on Artificial Intelligence (IJCAI 2025)<br>
	Neural Information Processing Systems (NeurIPS 2024)<br>
	European Conference on Artificial Intelligence (ECAI 2025)<br>
	<!-- Computer Vision and Pattern Recognition (CVPR)<br>
	International Conference on Computer Vision (ICCV)<br>
	European Conference on Computer Vision (ECCV)<br>
	Conference and Workshop on Neural Information Processing Systems (NeurIPS)<br>
	International Conference on Learning Representations (ICLR) <br>
	AAAI Conference on Artificial Intelligence (AAAI)<br>
	IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) -->
	</li>


</ul>


<!-- <h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2022-2023</td><td>Fall</td><td>Artificial Intelligence Technology (THU, 85990263-200)</td>
		</tr>
	</tbody>
</table> -->




<div id="footer">
	<div id="footer-text"></div>
</div>
	<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=777777&w=487&t=tt&d=xeJu_Kwek6AfO5eDCKFQ1iDWjzFQPLT_dNcYY3WLmrY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=5b1717'></script>
	<p><center> &copy; Jack Ma </center></p>


</div>
</body></html>
